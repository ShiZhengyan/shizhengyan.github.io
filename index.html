<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Zhengyan Shi</title>
    <meta name="author" content="Zhengyan Shi">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <!-- Replace SVG favicon with profile photo -->
    <link rel="icon" type="image/jpeg" href="asset/zhengyans.jpeg">
    <link rel="shortcut icon" type="image/jpeg" href="asset/zhengyans.jpeg">
    <!-- Add Apple Touch Icon for iOS devices -->
    <link rel="apple-touch-icon" href="asset/zhengyans.jpeg">
    
    <!-- Visitor tracking script -->
    <script data-goatcounter="https://zhengyanshi.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>
    <!-- 1️⃣ Modern fonts -->
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Roboto+Mono&display=swap" rel="stylesheet">
  </head>
  
  <body>
    <!-- theme-switcher -->
    <button id="theme-toggle" aria-label="Toggle dark / light">🌙</button>
    <main style="width:100%;max-width:800px;margin:0 auto;">
      <header style="padding:2.5%;">
        <table style="width:100%;border-spacing:0;">
          <tr>
            <td style="width:60%;vertical-align:middle">
              <h1 class="name" style="text-align:center;">Zhengyan Shi</h1>
              <p>
                Hi, welcome to my personal page.
                I am a Senior Researcher at <a href="https://www.microsoft.com/en-us/research/people/zhengyanshi/">Microsoft Research (MSR)</a>.
                <!-- I obtained my PhD from University College London, where I was supervised by <a href="https://aldolipani.com">Prof Aldo Lipani</a> and <a href="https://sites.google.com/site/emineyilmaz/">Prof Emine Yilmaz</a>.  -->
                I obtained my PhD in Computer Science at University College London (UCL). 
                Before that, I completed an MSc in Data Science (Statistics) with <i>Distinction</i> at UCL and a BSc in Mathematics with <i>First Class Honours</i> from the University of Liverpool and Xi'an Jiaotong-Liverpool University. 
                I have also held research internships at Cohere (London) and Amazon (London & Seattle).
              </p>
              <p>
                <!-- Prior to pursuing my PhD, I obtained a Master's degree in Data Science (Statistics) with <i>Distinction</i> from University College London and a Bachelor's degree in Mathematics with <i>First Class Honor</i> from University of Liverpool and Xi'an Jiaotong-Liverpool University.  -->
              </p>
              <br>
              <p>
                My current research at MSR focuses on teaching language models (LMs) to code. 
                I build learning loops in which LMs not only act but also reason within scalable, self-evolving environments. 
                By allowing models to plan, converse, and iterate inside these realistic sandboxes, I explore how LMs can continually refine themselves through interaction. 
                Central to my work is the ambition to leverage language models <strong>efficiently</strong> and <strong>robustly</strong> to solve <strong>general</strong> tasks.
                To that end, my existing work can be broadly categorized into the following directions:
              </p> <!-- close paragraph before the list -->
              <ul style="margin-left:1.5em;">
                <li>
                  <strong>Language Model Post-training</strong> 
                  [
                  <a href="https://arxiv.org/abs/2410.11677">Preprint 2024</a>,
                  <a href="https://nips.cc/virtual/2024/poster/95892">NeurIPS 2024</a>,
                  <a href="https://arxiv.org/abs/2309.05173">ICLR 2024</a>,
                  <a href="https://openreview.net/forum?id=s7xWeJQACI">NeurIPS 2023</a>,
                  <a href="https://aclanthology.org/2023.findings-acl.347/">Findings of ACL 2023</a>
                  ]
                </li>
                <li>
                  <strong>Interactive Systems</strong>
                  [
                  <a href="https://link.springer.com/chapter/10.1007/978-3-031-56027-9_1">ECIR 2024</a>,
                  <a href="https://aclanthology.org/2023.findings-emnlp.22/">Findings of EMNLP 2023</a>,
                  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21383">AAAI 2022</a>, 
                  <a href="https://aclanthology.org/2022.findings-naacl.158">Findings of NAACL 2022</a>,
                  <a href="https://nips.cc/virtual/2022/66405">NeurIPS WS 2022</a>
                  ]
                </li>
              </ul>
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=TF8l2ZEAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://twitter.com/Zhengyan_Shi">Twitter</a> &nbsp;/&nbsp;
                <a href="https://github.com/Shizhengyan/">Github</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/zhengyan-shi/">LinkedIn</a> &nbsp;/&nbsp;
                <a href="mailto:zhengyan.shi.19@gmail.com">Email</a>
              </p>
            </td>
            <td style="width:40%;max-width:40%;text-align:center;">
              <a href="asset/zhengyans.jpeg"><img style="width:75%;max-width:75%;border-radius:50%" alt="Profile photo" src="asset/zhengyans.jpeg"></a>
            </td>
          </tr>
        </table>
      </header>

      <section id="research">
        <h2 class="research-heading">Research (<i>Selected</i>)</h2>

        <style>
          .research-heading {
            padding-right: 1em; /* Adds space to the right of the text */
            margin-bottom: 1em; /* Adds space below the heading */
          }
        </style>
        <div class="research-papers">
          <article class="paper reveal">
            <div class="paper-image">
              <img src='asset/daa_likelihood.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://arxiv.org/abs/2410.11677">Understanding Likelihood Over-optimisation in Direct Alignment Algorithms</a></h3>
              <p class="authors">
                <strong>Zhengyan Shi</strong>,
                <a href="https://scholar.google.co.uk/citations?user=4pIBD4kAAAAJ">Sander Land</a>,
                <a href="https://scholar.google.co.uk/citations?user=pv4OI2EAAAAJ">Acyr Locatelli</a>,
                <a href="https://scholar.google.com/citations?user=ectPLEUAAAAJ">Matthieu Geist</a>,
                <a href="https://scholar.google.co.uk/citations?user=jPSWYn4AAAAJ">Max Bartolo</a>
              </p>
              <p class="publication">Preprint, 2024</p>
              <p class="links">
                <a href="https://arxiv.org/abs/2410.11677">Paper</a>
              </p>
              <p class="abstract">
                In this work, we identify a critical issue of likelihood over-optimisation in state-of-the-art Direct Alignment Algorithms (DAAs) and explore the relationship between completion likelihood and model performance.
              </p>
            </div>
          </article>

          <article class="paper reveal">
            <div class="paper-image">
              <img src='asset/insight_single.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://nips.cc/virtual/2024/poster/95892">Instruction Tuning With Loss Over Instructions</a></h3>
              <p class="authors">
                <strong>Zhengyan Shi</strong>,
                <a href="https://scholar.google.com/citations?user=KBNxz5sAAAAJ&hl=en">Adam X. Yang</a>,
                <a href="https://scholar.google.com/citations?user=2ZHjpDcAAAAJ&hl=en">Bin Wu</a>,
                <a href="https://scholar.google.com/citations?user=DF9khKUAAAAJ">Laurence Aitchison</a>,
                <a href="https://scholar.google.com/citations?user=ocmAN4YAAAAJ&hl=en">Emine Yilmaz</a>,
                <a href="https://scholar.google.com/citations?user=fyHjfEgAAAAJ&hl=en">Aldo Lipani</a>
              </p>
              <p class="publication">Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), 2024</p>
              <p class="links">
                <a href="https://arxiv.org/abs/2405.14394">Paper</a> /
                <a href="https://github.com/ZhengxiangShi/InstructionModelling">Github</a> /
                <a href="https://magazine.sebastianraschka.com/p/llm-research-insights-instruction">Community Discussion</a>
              </p>
              <p class="abstract">
                We show that in certain scenarios, applying loss to instructions rather than outputs only,
                which we refer to as Instruction Modelling, could largely improve the performance of instruction tuning on both various NLP and open-ended generation benchmarks. 
                Remarkably, in the most advantageous case, our approach boosts model performance on AlpacaEval 1.0 by over 100%.
              </p>
            </div>
          </article>
      
          <article class="paper reveal">
            <div class="paper-image">
              <img src='asset/dept.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://arxiv.org/abs/2309.05173">DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning</a></h3>
              <p class="authors">
                <strong>Zhengyan Shi</strong>,
                <a href="https://scholar.google.com/citations?user=fyHjfEgAAAAJ&hl=en">Aldo Lipani</a>
              </p>
              <p class="publication">International Conference on Learning Representations (<strong>ICLR</strong>), 2024</p>
              <p class="links">
                <a href="https://arxiv.org/abs/2309.05173">Paper</a> /
                <a href="https://github.com/ZhengxiangShi/DePT">Github</a> /
                <a href="https://x.com/arankomatsuzaki/status/1706313103372071110?s=20">Trending in Community</a>
              </p>
              <p class="abstract">
                Improves efficiency of Prompt Tuning in both time and memory by over 20% (when T5-Base is used as the backbone), with better performance. DePT grows more efficient as
                the model size increases.
              </p>
            </div>
          </article>
      
          <article class="paper reveal">
            <div class="paper-image">
              <img src='asset/preview.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://openreview.net/forum?id=s7xWeJQACI">Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner</a></h3>
              <p class="authors">
                <strong>Zhengyan Shi</strong>,
                <a href="https://aldolipani.com">Aldo Lipani</a>
              </p>
              <p class="publication">Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), 2023</p>
              <p class="links">
                <a href="https://openreview.net/forum?id=s7xWeJQACI">Paper</a> /
                <a href="https://github.com/ZhengxiangShi/PowerfulPromptFT">Github</a> /
                <a href="https://x.com/arankomatsuzaki/status/1654235182977757184?s=20">Trending in Community</a>
              </p>
              <p class="abstract">
                Combines the idea of the instruction tuning and language modelling. 
                Represents the first work to perform instruction tuning via unsupervised objectives.
                Boosts prompt-based fine-tuning performance by over 20% in absolute.
              </p>
            </div>
          </article>
      
          <article class="paper reveal">
            <div class="paper-image">
              <img src='asset/tapt_impr_5.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://aclanthology.org/2023.findings-acl.347/">Rethinking Semi-supervised Learning with Language Models</a></h3>
              <p class="authors">
                <strong>Zhengyan Shi</strong>,
                <a href="https://scholar.google.com/citations?user=4urrvVQAAAAJ&hl=en">Francesco Tonolini</a>,
                <a href="https://scholar.google.co.uk/citations?user=uxRWFhoAAAAJ&hl=en">Nikolaos Aletras</a>,
                <a href="https://scholar.google.com/citations?user=ocmAN4YAAAAJ&hl=en">Emine Yilmaz</a>,
                <a href="https://scholar.google.co.uk/citations?user=0U23qOUAAAAJ&hl=en">Gabriella Kazai</a>,
                <a href="https://scholar.google.com/citations?user=NgTM33MAAAAJ&hl=en">Yunlong Jiao</a>
              </p>
              <p class="publication">Association for Computational Linguistics (<strong>Findings of ACL</strong>), 2023</p>
              <p class="links">
                <a href="https://aclanthology.org/2023.findings-acl.347/">Paper</a> /
                <a href="https://github.com/amzn/pretraining-or-self-training">Github</a>
              </p>
              <p class="abstract">
                Shows Task-adaptive Pre-training (TAPT) as a simple yet effective method for semi-supervised learning (often SoTA performance).
                Highlights the effectiveness of TAPT even with only a few hundred unlabelled samples (in contrary to the common belief that continued pre-training requires a large amount of unlabelled data).
              </p>
            </div>
          </article>
      
          <article class="paper reveal">
            <div class="paper-image">
              <img src='asset/chain_construction.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://ojs.aaai.org/index.php/AAAI/article/view/21383">StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts</a></h3>
              <p class="authors">
                <strong>Zhengyan Shi</strong>,
                <a href="https://scholar.google.com/citations?user=ZKuRZaEAAAAJ">Qiang Zhang</a>,
                <a href="https://scholar.google.com/citations?user=fyHjfEgAAAAJ">Aldo Lipani</a>
              </p>
              <p class="publication">Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2022</p>
              <p class="links">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21383">Paper</a> /
                <a href="https://github.com/zhengxiangshi/stepgame">Github</a> /
                <a href="https://huggingface.co/datasets/michaelszx/StepGame">HuggingFace Dataset</a>
              </p>
              <p class="abstract">
                Introduces StepGame, a new benchmark for testing multi-hop spatial reasoning in texts. 
                This dataset challenges models to perform robust spatial reasoning across multiple steps, 
                providing a valuable tool for advancing natural language understanding in complex spatial scenarios.
              </p>
            </div>
          </article>
        </div>
      </section>
      
      <style>
        .research-papers {
          display: flex;
          flex-direction: column;
          gap: 2rem;
        }
        .paper {
          display: flex;
          gap: 1rem;
        }
        .paper-image {
          flex: 0 0 160px;
          display: flex;
          justify-content: center;
          align-items: center;
        }
        .paper-content {
          flex: 1;
        }
        .paper h3 {
          margin-top: 0;
        }
        .paper p {
          margin: 0.5em 0;
        }
        .authors, .publication, .links {
          font-size: 0.9em;
        }
        .abstract {
          margin-top: 1em;
        }
      </style>
      
      <script>
        /* 2️⃣ Keep the small helper only once */
        function toggleImageVisibility(imageId) {
          const image = document.getElementById(imageId);
          image.style.opacity = image.style.opacity === "0" ? "1" : "0";
        }
      </script>

      <section id="teaching-activities" class="content-section">
        <h2 class="section-heading">Teaching Activities</h2>
        <ul>
          <li>
            <strong>Guest Lecturer: Applied Artificial Intelligence</strong><br>
            University College London, Academic year 2023/24
          </li>
          <li>
            <strong>Guest Lecturer: Machine Learning for Data Science</strong><br>
            University College London, Academic year 2023/24
          </li>
          <li>
            <strong>Teaching Assistant: Statistical Natural Language Processing</strong><br>
            University College London, Academic year 2023/24
          </li>
          <li>
            <strong>Teaching Assistant: Machine Learning for Data Science</strong><br>
            University College London, Academic years 2020/21 - 2023/24
          </li>
          <li>
            <strong>Teaching Assistant: Geospatial Programming</strong><br>
            University College London, Academic years 2020/21 - 2023/24
          </li>
          <li>
            <strong>Co-supervisor: MSc Research Project</strong><br>
            University College London, Academic year 2022/23 - 2023/24
          </li>
        </ul>
      </section>
      
      <section id="academic-services" class="content-section">
        <h2 class="section-heading">Academic Services</h2>
        <p><strong>Program Committee</strong>: NeurIPS (2023, 2024), ICML (2024), ICLR (2025), AAAI (2023, 2024), COLM (2024), ACL ARR (Feb. 2023 - Jan. 2024), ACL (2023), EMNLP (2022, 2023), EACL (2023), COLING (2023, 2024), ECML/PKDD (2022), KDD (2023), SIGIR (2022, 2023, 2024), ECIR (2024), SDM (2024)</p>
      </section>
      
      <style>
        .content-section {
          margin-top: 3em;  /* Adds space above each section */
          padding-top: 1em; /* Adds some padding inside the top of the section */
        }
        .section-heading {
          margin-bottom: 1em; /* Adds space below the heading */
        }
      </style>

    </main>

    <!-- 3️⃣ Reveal-on-scroll (unchanged) -->
    <script>
// filepath: inline-reveal-observer
document.addEventListener('DOMContentLoaded', () => {
  const elements = document.querySelectorAll('.reveal');
  const io = new IntersectionObserver(
    entries => entries.forEach(e => e.isIntersecting && e.target.classList.add('visible')),
    { threshold: 0.1 }
  );
  elements.forEach(el => io.observe(el));
});
    </script>

    <!-- 4️⃣ Theme initialisation & persistence -->
    <script>
// filepath: inline-theme-toggle
(function () {
  const root = document.documentElement;
  const btn  = document.getElementById('theme-toggle');
  const saved = localStorage.getItem('theme');
  const systemDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
  if (saved === 'dark' || (!saved && systemDark)) {
    root.classList.add('dark');
    btn.textContent = '☀️';
  }
  btn.addEventListener('click', () => {
    const dark = root.classList.toggle('dark');
    localStorage.setItem('theme', dark ? 'dark' : 'light');
    btn.textContent = dark ? '☀️' : '🌙';
  });
})();
    </script>

    <!-- 🧑‍🔬 Lightweight AI research agent -->
    <div id="ai-agent" class="reveal">
      <div class="agent-avatar" aria-label="AI agent">🤖</div>
      <ul class="agent-menu">
        <li><button data-action="pubs">View recent publications</button></li>
        <li><button data-action="teach">Learn about teaching & courses</button></li>
        <li><button data-action="intro">Read short research intro</button></li>
        <li><button data-action="quote">Show an AI quote</button></li>
        <li><button data-action="chat">Ask me anything</button></li> <!-- NEW -->
      </ul>

      <!-- 💬 light chat UI -->
      <div id="agent-chat" hidden>
        <div class="chat-messages"></div>
        <div class="chat-input">
          <textarea rows="1" placeholder="Ask me about my research…"></textarea>
          <button class="send-btn" aria-label="Send">➤</button>
        </div>
      </div>

      <p class="agent-quote" hidden></p>
    </div>

    <script>
// filepath: inline-ai-agent
(function () {
  const agent      = document.getElementById('ai-agent');
  const avatar     = agent.querySelector('.agent-avatar');
  const menu       = agent.querySelector('.agent-menu');
  const quoteBox   = agent.querySelector('.agent-quote');
  const chatBox     = document.getElementById('agent-chat');
  const msgArea     = chatBox.querySelector('.chat-messages');
  const input       = chatBox.querySelector('textarea');
  const sendBtn     = chatBox.querySelector('.send-btn');

  const quotes = [
    "The future is already here — it's just not evenly distributed. – W. Gibson",
    "Intelligence is the ability to avoid doing work, yet getting the work done. – L. Hubbard",
    "What we can do with AI will redefine what it means to be human.",
    "The question is not can machines think, but can they dream?"
  ];

  /* ---------- helpers ---------- */
  const append = (role, text) => {
    const el = document.createElement('div');
    el.className = `chat-msg ${role}`;
    el.textContent = text;
    msgArea.appendChild(el);
    msgArea.scrollTop = msgArea.scrollHeight;
  };

  async function fetchLLMResponse(q){
    const apiKey = window.AI_AGENT_API_KEY;      // ← set this for OpenRouter
    /* ---------- helper for local rule-based reply ---------- */
    const localReply = txt => {
      txt = txt.toLowerCase();
      if(/hi|hello|hey/.test(txt)) return "Hello! 👋 How can I help you with my research?";
      if(/(paper|publication)/.test(txt)) return "You can scroll to the Research section for selected papers. Let me know if you want details on a specific one.";
      if(/teach|course/.test(txt)) return "I regularly teach and guest-lecture at UCL. Scroll to the Teaching section for a list.";
      if(/research|work|focus/.test(txt)) return "I study how to teach language models to code and reason in self-evolving environments.";
      if(/email|contact/.test(txt)) return "Feel free to email me at zhengyan.shi.19@gmail.com.";
      return "I'm a lightweight local bot. Try asking about my papers, teaching, or research!";
    };

    /* ---------- 1️⃣ OpenRouter if key present ---------- */
    if (apiKey){
      try{
        const res = await fetch('https://openrouter.ai/api/v1/chat/completions',{
          method:'POST',
          headers:{
            'Content-Type':'application/json',
            'Authorization':`Bearer ${apiKey}`
          },
          body:JSON.stringify({
            model:"openai/gpt-3.5-turbo",
            messages:[{role:"user",content:q}],
            max_tokens:150
          })
        });
        const data = await res.json();
        const answer = data.choices?.[0]?.message?.content?.trim();
        if (answer) return answer;
      }catch{/* fall through to other options */}
    }

    /* ---------- 2️⃣ Free public endpoint (no key) ---------- */
    try{
      const url = `https://api.affiliateplus.xyz/api/chatbot?message=${encodeURIComponent(q)}&owner=zy&botname=ZyBot`;
      const r   = await fetch(url);
      if(r.ok){
        const j = await r.json();
        return j.message?.trim() || localReply(q);
      }
    }catch{/* ignore & fallback */ }

    /* ---------- 3️⃣ Local rule-based ---------- */
    return localReply(q);
  }

  /* ---------- UI interactions ---------- */
  avatar.addEventListener('click',()=>menu.classList.toggle('open'));

  menu.addEventListener('click', e => {
    if(e.target.tagName!=='BUTTON') return;
    const action=e.target.dataset.action;
    menu.classList.remove('open');
    quoteBox.hidden=true;

    if(action==='chat'){ chatBox.hidden=!chatBox.hidden; input.focus(); return; }

    const scrollTo=id=>document.getElementById(id)
        .scrollIntoView({behavior:'smooth',block:'start'});
    if(action==='pubs')  scrollTo('research');
    if(action==='teach') scrollTo('teaching-activities');
    if(action==='intro') window.scrollTo({top:0,behavior:'smooth'});
    if(action==='quote'){
      quoteBox.textContent=quotes[Math.random()*quotes.length|0];
      quoteBox.hidden=false;
    }
  });

  const send = async()=>{
    const q=input.value.trim();
    if(!q) return;
    append('user',q);
    input.value=''; input.focus();
    append('ai','…');                           // typing indicator
    const typingEl = msgArea.lastChild;
    const a=await fetchLLMResponse(q);
    typingEl.textContent=a;
  };
  sendBtn.addEventListener('click',send);
  input.addEventListener('keydown',e=>{ if(e.key==='Enter'&&!e.shiftKey){e.preventDefault();send();} });
})();
    </script>
  </body>
</html>