<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Zhengyan Shi</title>
    <meta name="author" content="Zhengyan Shi">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <!-- Replace SVG favicon with profile photo -->
    <link rel="icon" type="image/jpeg" href="asset/zhengyans.jpeg">
    <link rel="shortcut icon" type="image/jpeg" href="asset/zhengyans.jpeg">
    <!-- Add Apple Touch Icon for iOS devices -->
    <link rel="apple-touch-icon" href="asset/zhengyans.jpeg">
    
    <!-- Visitor tracking script -->
    <script data-goatcounter="https://zhengyanshi.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>
    <!-- 1️⃣ Modern fonts -->
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Roboto+Mono&display=swap" rel="stylesheet">
  </head>
  
  <body>
    <!-- theme-switcher -->
    <button id="theme-toggle" aria-label="Toggle dark / light">🌙</button>
    <main style="width:100%;max-width:800px;margin:0 auto;">
      <header style="padding:2.5%;">
        <table style="width:100%;border-spacing:0;">
          <tr>
            <td style="width:60%;vertical-align:middle">
              <h1 class="name" style="text-align:center;">Zhengyan Shi</h1>
              <p>
                Hi, welcome to my personal page.
                I am a Senior Researcher at <a href="https://www.microsoft.com/en-us/research/people/zhengyanshi/">Microsoft Research (MSR)</a>.
                <!-- I obtained my PhD from University College London, where I was supervised by <a href="https://aldolipani.com">Prof Aldo Lipani</a> and <a href="https://sites.google.com/site/emineyilmaz/">Prof Emine Yilmaz</a>.  -->
                I obtained my PhD in Computer Science at University College London (UCL). 
                Before that, I completed an MSc in Data Science (Statistics) with <i>Distinction</i> at UCL and a BSc in Mathematics with <i>First Class Honours</i> from the University of Liverpool and Xi'an Jiaotong-Liverpool University. 
                I have also held research internships at Cohere (London) and Amazon (London & Seattle).
              </p>
              <p>
                <!-- Prior to pursuing my PhD, I obtained a Master's degree in Data Science (Statistics) with <i>Distinction</i> from University College London and a Bachelor's degree in Mathematics with <i>First Class Honor</i> from University of Liverpool and Xi'an Jiaotong-Liverpool University.  -->
              </p>
              <br>
              <p>
                My current research at MSR focuses on teaching language models (LMs) to code. 
                I build learning loops in which LMs not only act but also reason within scalable, self-evolving environments. 
                By allowing models to plan, converse, and iterate inside these realistic sandboxes, I explore how LMs can continually refine themselves through interaction. 
                Central to my work is the ambition to leverage language models <strong>efficiently</strong> and <strong>robustly</strong> to solve <strong>general</strong> tasks.
                To that end, my existing work can be broadly categorized into the following directions:
              </p> <!-- close paragraph before the list -->
              <ul style="margin-left:1.5em;">
                <li>
                  <strong>Language Model Post-training</strong> 
                  [
                  <a href="https://arxiv.org/abs/2410.11677">Preprint 2024</a>,
                  <a href="https://nips.cc/virtual/2024/poster/95892">NeurIPS 2024</a>,
                  <a href="https://arxiv.org/abs/2309.05173">ICLR 2024</a>,
                  <a href="https://openreview.net/forum?id=s7xWeJQACI">NeurIPS 2023</a>,
                  <a href="https://aclanthology.org/2023.findings-acl.347/">Findings of ACL 2023</a>
                  ]
                </li>
                <li>
                  <strong>Interactive Systems</strong>
                  [
                  <a href="https://link.springer.com/chapter/10.1007/978-3-031-56027-9_1">ECIR 2024</a>,
                  <a href="https://aclanthology.org/2023.findings-emnlp.22/">Findings of EMNLP 2023</a>,
                  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21383">AAAI 2022</a>, 
                  <a href="https://aclanthology.org/2022.findings-naacl.158">Findings of NAACL 2022</a>,
                  <a href="https://nips.cc/virtual/2022/66405">NeurIPS WS 2022</a>
                  ]
                </li>
              </ul>
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=TF8l2ZEAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://twitter.com/Zhengyan_Shi">Twitter</a> &nbsp;/&nbsp;
                <a href="https://github.com/Shizhengyan/">Github</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/zhengyan-shi/">LinkedIn</a> &nbsp;/&nbsp;
                <a href="mailto:zhengyan.shi.19@gmail.com">Email</a>
              </p>
            </td>
            <td style="width:40%;max-width:40%;text-align:center;">
              <a href="asset/zhengyans.jpeg"><img style="width:75%;max-width:75%;border-radius:50%" alt="Zhengyan Shi - Senior Researcher at Microsoft Research" src="asset/zhengyans.jpeg" loading="lazy"></a>
            </td>
          </tr>
        </table>
      </header>

      <section id="research">
        <h2 class="research-heading">Research (<i>Selected</i>)</h2>

        <style>
          .research-heading {
            padding-right: 1em; /* Adds space to the right of the text */
            margin-bottom: 1em; /* Adds space below the heading */
          }
        </style>
        <div class="research-papers">
          <article class="paper reveal">
            <div class="paper-image">
              <img src='asset/daa_likelihood.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://arxiv.org/abs/2410.11677">Understanding Likelihood Over-optimisation in Direct Alignment Algorithms</a></h3>
              <p class="authors">
                <strong>Zhengyan Shi</strong>,
                <a href="https://scholar.google.co.uk/citations?user=4pIBD4kAAAAJ">Sander Land</a>,
                <a href="https://scholar.google.co.uk/citations?user=pv4OI2EAAAAJ">Acyr Locatelli</a>,
                <a href="https://scholar.google.com/citations?user=ectPLEUAAAAJ">Matthieu Geist</a>,
                <a href="https://scholar.google.co.uk/citations?user=jPSWYn4AAAAJ">Max Bartolo</a>
              </p>
              <p class="publication">Preprint, 2024</p>
              <p class="links">
                <a href="https://arxiv.org/abs/2410.11677">Paper</a>
              </p>
              <p class="abstract">
                In this work, we identify a critical issue of likelihood over-optimisation in state-of-the-art Direct Alignment Algorithms (DAAs) and explore the relationship between completion likelihood and model performance.
              </p>
            </div>
          </article>

          <article class="paper reveal">
            <div class="paper-image">
              <img src='asset/insight_single.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://nips.cc/virtual/2024/poster/95892">Instruction Tuning With Loss Over Instructions</a></h3>
              <p class="authors">
                <strong>Zhengyan Shi</strong>,
                <a href="https://scholar.google.com/citations?user=KBNxz5sAAAAJ&hl=en">Adam X. Yang</a>,
                <a href="https://scholar.google.com/citations?user=2ZHjpDcAAAAJ&hl=en">Bin Wu</a>,
                <a href="https://scholar.google.com/citations?user=DF9khKUAAAAJ">Laurence Aitchison</a>,
                <a href="https://scholar.google.com/citations?user=ocmAN4YAAAAJ&hl=en">Emine Yilmaz</a>,
                <a href="https://scholar.google.com/citations?user=fyHjfEgAAAAJ&hl=en">Aldo Lipani</a>
              </p>
              <p class="publication">Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), 2024</p>
              <p class="links">
                <a href="https://arxiv.org/abs/2405.14394">Paper</a> /
                <a href="https://github.com/ZhengxiangShi/InstructionModelling">Github</a> /
                <a href="https://magazine.sebastianraschka.com/p/llm-research-insights-instruction">Community Discussion</a>
              </p>
              <p class="abstract">
                We show that in certain scenarios, applying loss to instructions rather than outputs only,
                which we refer to as Instruction Modelling, could largely improve the performance of instruction tuning on both various NLP and open-ended generation benchmarks. 
                Remarkably, in the most advantageous case, our approach boosts model performance on AlpacaEval 1.0 by over 100%.
              </p>
            </div>
          </article>
      
          <article class="paper reveal">
            <div class="paper-image">
              <img src='asset/dept.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://arxiv.org/abs/2309.05173">DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning</a></h3>
              <p class="authors">
                <strong>Zhengyan Shi</strong>,
                <a href="https://scholar.google.com/citations?user=fyHjfEgAAAAJ&hl=en">Aldo Lipani</a>
              </p>
              <p class="publication">International Conference on Learning Representations (<strong>ICLR</strong>), 2024</p>
              <p class="links">
                <a href="https://arxiv.org/abs/2309.05173">Paper</a> /
                <a href="https://github.com/ZhengxiangShi/DePT">Github</a> /
                <a href="https://x.com/arankomatsuzaki/status/1706313103372071110?s=20">Trending in Community</a>
              </p>
              <p class="abstract">
                Improves efficiency of Prompt Tuning in both time and memory by over 20% (when T5-Base is used as the backbone), with better performance. DePT grows more efficient as
                the model size increases.
              </p>
            </div>
          </article>
      
          <article class="paper reveal">
            <div class="paper-image">
              <img src='asset/preview.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://openreview.net/forum?id=s7xWeJQACI">Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner</a></h3>
              <p class="authors">
                <strong>Zhengyan Shi</strong>,
                <a href="https://aldolipani.com">Aldo Lipani</a>
              </p>
              <p class="publication">Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), 2023</p>
              <p class="links">
                <a href="https://openreview.net/forum?id=s7xWeJQACI">Paper</a> /
                <a href="https://github.com/ZhengxiangShi/PowerfulPromptFT">Github</a> /
                <a href="https://x.com/arankomatsuzaki/status/1654235182977757184?s=20">Trending in Community</a>
              </p>
              <p class="abstract">
                Combines the idea of the instruction tuning and language modelling. 
                Represents the first work to perform instruction tuning via unsupervised objectives.
                Boosts prompt-based fine-tuning performance by over 20% in absolute.
              </p>
            </div>
          </article>
      
          <article class="paper reveal">
            <div class="paper-image">
              <img src='asset/tapt_impr_5.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://aclanthology.org/2023.findings-acl.347/">Rethinking Semi-supervised Learning with Language Models</a></h3>
              <p class="authors">
                <strong>Zhengyan Shi</strong>,
                <a href="https://scholar.google.com/citations?user=4urrvVQAAAAJ&hl=en">Francesco Tonolini</a>,
                <a href="https://scholar.google.co.uk/citations?user=uxRWFhoAAAAJ&hl=en">Nikolaos Aletras</a>,
                <a href="https://scholar.google.com/citations?user=ocmAN4YAAAAJ&hl=en">Emine Yilmaz</a>,
                <a href="https://scholar.google.co.uk/citations?user=0U23qOUAAAAJ&hl=en">Gabriella Kazai</a>,
                <a href="https://scholar.google.com/citations?user=NgTM33MAAAAJ&hl=en">Yunlong Jiao</a>
              </p>
              <p class="publication">Association for Computational Linguistics (<strong>Findings of ACL</strong>), 2023</p>
              <p class="links">
                <a href="https://aclanthology.org/2023.findings-acl.347/">Paper</a> /
                <a href="https://github.com/amzn/pretraining-or-self-training">Github</a>
              </p>
              <p class="abstract">
                Shows Task-adaptive Pre-training (TAPT) as a simple yet effective method for semi-supervised learning (often SoTA performance).
                Highlights the effectiveness of TAPT even with only a few hundred unlabelled samples (in contrary to the common belief that continued pre-training requires a large amount of unlabelled data).
              </p>
            </div>
          </article>
      
          <article class="paper reveal">
            <div class="paper-image">
              <img src='asset/chain_construction.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://ojs.aaai.org/index.php/AAAI/article/view/21383">StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts</a></h3>
              <p class="authors">
                <strong>Zhengyan Shi</strong>,
                <a href="https://scholar.google.com/citations?user=ZKuRZaEAAAAJ">Qiang Zhang</a>,
                <a href="https://scholar.google.com/citations?user=fyHjfEgAAAAJ">Aldo Lipani</a>
              </p>
              <p class="publication">Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2022</p>
              <p class="links">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21383">Paper</a> /
                <a href="https://github.com/zhengxiangshi/stepgame">Github</a> /
                <a href="https://huggingface.co/datasets/michaelszx/StepGame">HuggingFace Dataset</a>
              </p>
              <p class="abstract">
                Introduces StepGame, a new benchmark for testing multi-hop spatial reasoning in texts. 
                This dataset challenges models to perform robust spatial reasoning across multiple steps, 
                providing a valuable tool for advancing natural language understanding in complex spatial scenarios.
              </p>
            </div>
          </article>
        </div>
      </section>
      
      <style>
        .research-papers {
          display: flex;
          flex-direction: column;
          gap: 2rem;
        }
        .paper {
          display: flex;
          gap: 1rem;
        }
        .paper-image {
          flex: 0 0 160px;
          display: flex;
          justify-content: center;
          align-items: center;
        }
        .paper-content {
          flex: 1;
        }
        .paper h3 {
          margin-top: 0;
        }
        .paper p {
          margin: 0.5em 0;
        }
        .authors, .publication, .links {
          font-size: 0.9em;
        }
        .abstract {
          margin-top: 1em;
        }
      </style>
      
      <script>
        /* 2️⃣ Keep the small helper only once */
        function toggleImageVisibility(imageId) {
          const image = document.getElementById(imageId);
          image.style.opacity = image.style.opacity === "0" ? "1" : "0";
        }
      </script>

      <section id="teaching-activities" class="content-section">
        <h2 class="section-heading">Teaching Activities</h2>
        <ul>
          <li>
            <strong>Guest Lecturer: Applied Artificial Intelligence</strong><br>
            University College London, Academic year 2023/24
          </li>
          <li>
            <strong>Guest Lecturer: Machine Learning for Data Science</strong><br>
            University College London, Academic year 2023/24
          </li>
          <li>
            <strong>Teaching Assistant: Statistical Natural Language Processing</strong><br>
            University College London, Academic year 2023/24
          </li>
          <li>
            <strong>Teaching Assistant: Machine Learning for Data Science</strong><br>
            University College London, Academic years 2020/21 - 2023/24
          </li>
          <li>
            <strong>Teaching Assistant: Geospatial Programming</strong><br>
            University College London, Academic years 2020/21 - 2023/24
          </li>
          <li>
            <strong>Co-supervisor: MSc Research Project</strong><br>
            University College London, Academic year 2022/23 - 2023/24
          </li>
        </ul>
      </section>

      <section id="news" class="content-section reveal">
        <h2 class="section-heading">Recent News</h2>
        <div class="news-container">
          <div class="news-item">
            <span class="news-date">2024/10</span>
            <span class="news-content">New preprint on likelihood over-optimisation in direct alignment algorithms is now available on <a href="https://arxiv.org/abs/2410.11677">arXiv</a>.</span>
          </div>
          <div class="news-item">
            <span class="news-date">2024/09</span>
            <span class="news-content">Paper on "Instruction Tuning With Loss Over Instructions" accepted to <strong>NeurIPS 2024</strong>!</span>
          </div>
          <div class="news-item">
            <span class="news-date">2024/01</span>
            <span class="news-content">Paper on "DePT: Decomposed Prompt Tuning" accepted to <strong>ICLR 2024</strong>!</span>
          </div>
          <div class="news-item">
            <span class="news-date">2023/09</span>
            <span class="news-content">Paper accepted to <strong>NeurIPS 2023</strong> on powerful prompt-based fine-tuning!</span>
          </div>
        </div>
      </section>
      
      <section id="academic-services" class="content-section">
        <h2 class="section-heading">Academic Services</h2>
        <p><strong>Program Committee</strong>: NeurIPS (2023, 2024), ICML (2024), ICLR (2025), AAAI (2023, 2024), COLM (2024), ACL ARR (Feb. 2023 - Jan. 2024), ACL (2023), EMNLP (2022, 2023), EACL (2023), COLING (2023, 2024), ECML/PKDD (2022), KDD (2023), SIGIR (2022, 2023, 2024), ECIR (2024), SDM (2024)</p>
      </section>
      
      <style>
        .content-section {
          margin-top: 3em;  /* Adds space above each section */
          padding-top: 1em; /* Adds some padding inside the top of the section */
        }
        .section-heading {
          margin-bottom: 1em; /* Adds space below the heading */
        }
      </style>

    </main>

    <!-- 3️⃣ Reveal-on-scroll (unchanged) -->
    <script>
// filepath: inline-reveal-observer
document.addEventListener('DOMContentLoaded', () => {
  const elements = document.querySelectorAll('.reveal');
  const io = new IntersectionObserver(
    entries => entries.forEach(e => e.isIntersecting && e.target.classList.add('visible')),
    { threshold: 0.1 }
  );
  elements.forEach(el => io.observe(el));
});
    </script>

    <!-- 4️⃣ Theme initialisation & persistence -->
    <script>
// filepath: inline-theme-toggle
(function () {
  const root = document.documentElement;
  const btn  = document.getElementById('theme-toggle');
  const saved = localStorage.getItem('theme');
  const systemDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
  if (saved === 'dark' || (!saved && systemDark)) {
    root.classList.add('dark');
    btn.textContent = '☀️';
  }
  btn.addEventListener('click', () => {
    const dark = root.classList.toggle('dark');
    localStorage.setItem('theme', dark ? 'dark' : 'light');
    btn.textContent = dark ? '☀️' : '🌙';
  });
})();
    </script>

    <!-- 🧑‍🚀 Floating Astronaut Explorer -->
    <div class="floating-astronaut" id="astronaut">🧑‍🚀</div>
    <div class="background-stars"></div>
    
    <!-- 🎮 Subtle instructions tooltip -->
    <div id="astronaut-hint" class="astronaut-hint">
      <span>Use arrow keys to move the astronaut! 🧑‍🚀</span>
    </div>

    <!-- 🧑‍🔬 Lightweight AI research agent -->
    <div id="ai-agent" class="reveal">
      <div class="agent-avatar" aria-label="AI agent">🤖</div>
      <ul class="agent-menu">
        <li><button data-action="pubs">View recent publications</button></li>
        <li><button data-action="teach">Learn about teaching & courses</button></li>
        <li><button data-action="intro">Read short research intro</button></li>
        <li><button data-action="quote">Show an AI quote</button></li>
        <li><button data-action="chat">Ask me anything</button></li> <!-- NEW -->
      </ul>

      <!-- 💬 light chat UI -->
      <div id="agent-chat" hidden>
        <div class="chat-messages"></div>
        <div class="chat-input">
          <textarea rows="1" placeholder="Ask me about my research…"></textarea>
          <button class="send-btn" aria-label="Send">➤</button>
        </div>
      </div>

      <p class="agent-quote" hidden></p>
    </div>

    <script>
// filepath: inline-ai-agent
(function () {
  const agent      = document.getElementById('ai-agent');
  const avatar     = agent.querySelector('.agent-avatar');
  const menu       = agent.querySelector('.agent-menu');
  const quoteBox   = agent.querySelector('.agent-quote');
  const chatBox     = document.getElementById('agent-chat');
  const msgArea     = chatBox.querySelector('.chat-messages');
  const input       = chatBox.querySelector('textarea');
  const sendBtn     = chatBox.querySelector('.send-btn');

  const quotes = [
    "The future is already here — it's just not evenly distributed. – W. Gibson",
    "Intelligence is the ability to avoid doing work, yet getting the work done. – L. Hubbard",
    "What we can do with AI will redefine what it means to be human.",
    "The question is not can machines think, but can they dream?"
  ];

  /* ---------- helpers ---------- */
  const append = (role, text) => {
    const el = document.createElement('div');
    el.className = `chat-msg ${role}`;
    el.textContent = text;
    msgArea.appendChild(el);
    msgArea.scrollTop = msgArea.scrollHeight;
  };

  async function fetchLLMResponse(q){
    /* ---------- Enhanced context about Zhengyan Shi ---------- */
    const contextPrompt = `You are an AI assistant representing Zhengyan Shi, a Senior Researcher at Microsoft Research. Here's information about him:

BACKGROUND:
- Senior Researcher at Microsoft Research (MSR)
- PhD in Computer Science from University College London (UCL)
- MSc in Data Science (Statistics) with Distinction from UCL
- BSc in Mathematics with First Class Honours from University of Liverpool and Xi'an Jiaotong-Liverpool University
- Research internships at Cohere (London) and Amazon (London & Seattle)

RESEARCH FOCUS:
- Teaching language models (LMs) to code
- Building learning loops where LMs act and reason in scalable, self-evolving environments
- Language model post-training and instruction tuning
- Interactive systems and prompt-based fine-tuning
- Making language models work efficiently, robustly, and generally

RECENT WORK:
- "Understanding Likelihood Over-optimisation in Direct Alignment Algorithms" (2024 preprint)
- "Instruction Tuning With Loss Over Instructions" (NeurIPS 2024)
- "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning" (ICLR 2024)
- "Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner" (NeurIPS 2023)

Question: ${q}

Please respond as if you're an AI assistant representing Zhengyan, providing helpful information about his research, background, or related topics. Keep responses concise (1-3 sentences) and friendly.`;

    /* ---------- Multiple free LLM APIs ---------- */
    const apis = [
      // Hugging Face Inference API (free tier)
      {
        url: 'https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ inputs: contextPrompt })
      },
      
      // Alternative free API
      {
        url: `https://api.affiliateplus.xyz/api/chatbot?message=${encodeURIComponent(contextPrompt)}&owner=zhengyan&botname=ResearchBot`
      },
      
      // Another free option
      {
        url: 'https://api.cohere.ai/v1/generate',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: 'command-light',
          prompt: contextPrompt,
          max_tokens: 200,
          temperature: 0.7
        })
      }
    ];

    /* ---------- Try multiple APIs ---------- */
    for (const api of apis) {
      try {
        const response = await fetch(api.url, {
          method: api.body ? 'POST' : 'GET',
          headers: api.headers || {},
          body: api.body
        });
        
        if (response.ok) {
          const data = await response.json();
          
          // Handle different response formats
          let answer = '';
          if (data.generated_text) answer = data.generated_text;
          else if (data.message) answer = data.message;
          else if (data.generations?.[0]?.text) answer = data.generations[0].text;
          else if (Array.isArray(data) && data[0]?.generated_text) answer = data[0].generated_text;
          
          if (answer && answer.trim()) {
            return answer.trim().substring(0, 300); // Limit response length
          }
        }
      } catch (error) {
        console.log('API attempt failed, trying next...');
        continue;
      }
    }

    /* ---------- Enhanced local rule-based responses ---------- */
    const txt = q.toLowerCase();
    
    if(/hi|hello|hey|greet/.test(txt)) {
      return "Hello! 👋 I'm here to help you learn about Zhengyan's research in language models and AI. What would you like to know?";
    }
    
    if(/(paper|publication|research|work)/.test(txt)) {
      return "Zhengyan focuses on teaching language models to code and reason. His recent work includes instruction tuning (NeurIPS 2024), DePT for efficient fine-tuning (ICLR 2024), and likelihood over-optimization in alignment algorithms. Which area interests you most?";
    }
    
    if(/teach|course|education/.test(txt)) {
      return "Zhengyan teaches at UCL including Applied AI, Machine Learning for Data Science, and Statistical NLP. He also co-supervises MSc research projects. Are you interested in any specific course?";
    }
    
    if(/microsoft|msr|job|career/.test(txt)) {
      return "At Microsoft Research, Zhengyan builds learning loops where language models act and reason in self-evolving environments. He's passionate about making LMs work efficiently and robustly for general tasks.";
    }
    
    if(/contact|email|connect/.test(txt)) {
      return "You can reach Zhengyan at zhengyan.shi.19@gmail.com or connect with him on LinkedIn, Twitter (@Zhengyan_Shi), or GitHub. He's always open to discussing research collaborations!";
    }
    
    if(/background|education|phd/.test(txt)) {
      return "Zhengyan has a PhD from UCL, MSc in Data Science with Distinction, and BSc in Mathematics with First Class Honours. He's also done internships at Cohere and Amazon before joining Microsoft Research.";
    }
    
    return "I'm an AI assistant representing Zhengyan Shi, a researcher at Microsoft Research working on language models. Feel free to ask about his research, papers, teaching, or background! 🤖";
  }

  /* ---------- UI interactions ---------- */
  avatar.addEventListener('click',()=>menu.classList.toggle('open'));

  menu.addEventListener('click', e => {
    if(e.target.tagName!=='BUTTON') return;
    const action=e.target.dataset.action;
    menu.classList.remove('open');
    quoteBox.hidden=true;

    if(action==='chat'){ chatBox.hidden=!chatBox.hidden; input.focus(); return; }

    const scrollTo=id=>document.getElementById(id)
        .scrollIntoView({behavior:'smooth',block:'start'});
    if(action==='pubs')  scrollTo('research');
    if(action==='teach') scrollTo('teaching-activities');
    if(action==='intro') window.scrollTo({top:0,behavior:'smooth'});
    if(action==='quote'){
      quoteBox.textContent=quotes[Math.random()*quotes.length|0];
      quoteBox.hidden=false;
    }
  });

  const send = async()=>{
    const q=input.value.trim();
    if(!q) return;
    append('user',q);
    input.value=''; input.focus();
    append('ai','…');                           // typing indicator
    const typingEl = msgArea.lastChild;
    const a=await fetchLLMResponse(q);
    typingEl.textContent=a;
  };
  sendBtn.addEventListener('click',send);
  input.addEventListener('keydown',e=>{ if(e.key==='Enter'&&!e.shiftKey){e.preventDefault();send();} });
})();
    </script>

    <!-- 👨‍🚀 Floating Astronaut Controller Script -->
    <script>
// filepath: inline-floating-astronaut
(function () {
  const astronaut = document.getElementById('astronaut');
  const backgroundStars = document.querySelector('.background-stars');
  const hint = document.getElementById('astronaut-hint');
  
  let astronautPos = { x: 20, y: 20 }; // percentage positions
  let keys = {};
  let isMoving = false;
  let bgStars = [];
  
  // Initialize
  function init() {
    createBackgroundStars();
    positionAstronaut();
    
    // Start the game loop
    gameLoop();
  }
  
  // Create subtle background stars
  function createBackgroundStars() {
    backgroundStars.innerHTML = '';
    bgStars = [];
    
    for (let i = 0; i < 50; i++) {
      const star = document.createElement('div');
      star.className = 'bg-star';
      
      const size = Math.random() * 2 + 1; // 1-3px stars
      const x = Math.random() * 100;
      const y = Math.random() * 100;
      const delay = Math.random() * 3;
      
      star.style.cssText = `
        width: ${size}px;
        height: ${size}px;
        left: ${x}%;
        top: ${y}%;
        animation-delay: ${delay}s;
      `;
      
      backgroundStars.appendChild(star);
      bgStars.push({ element: star, x, y, size });
    }
  }
  
  // Position astronaut
  function positionAstronaut() {
    astronaut.style.left = astronautPos.x + '%';
    astronaut.style.top = astronautPos.y + '%';
  }
  
  // Create particle trail
  function createParticle() {
    const particle = document.createElement('div');
    particle.className = 'particle';
    
    const rect = astronaut.getBoundingClientRect();
    particle.style.left = (rect.left + rect.width / 2) + 'px';
    particle.style.top = (rect.top + rect.height / 2) + 'px';
    
    document.body.appendChild(particle);
    
    // Remove particle after animation
    setTimeout(() => {
      if (particle.parentNode) {
        particle.parentNode.removeChild(particle);
      }
    }, 1000);
  }
  
  // Movement system with page scrolling
  function updateAstronautPosition() {
    let moved = false;
    const speed = 1.5; // percentage per frame
    const scrollSpeed = 15; // pixels to scroll
    
    // Get current scroll position and page dimensions
    const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
    const scrollLeft = window.pageXOffset || document.documentElement.scrollLeft;
    const windowHeight = window.innerHeight;
    const windowWidth = window.innerWidth;
    const documentHeight = Math.max(document.body.scrollHeight, document.documentElement.scrollHeight);
    const documentWidth = Math.max(document.body.scrollWidth, document.documentElement.scrollWidth);
    
    if (keys['ArrowUp'] || keys['KeyW']) {
      if (astronautPos.y <= 2) {
        // At top boundary - scroll up if possible
        if (scrollTop > 0) {
          window.scrollBy(0, -scrollSpeed);
          moved = true;
        }
      } else {
        astronautPos.y = Math.max(2, astronautPos.y - speed);
        moved = true;
      }
    }
    
    if (keys['ArrowDown'] || keys['KeyS']) {
      if (astronautPos.y >= 85) {
        // At bottom boundary - scroll down if possible
        if (scrollTop < documentHeight - windowHeight) {
          window.scrollBy(0, scrollSpeed);
          moved = true;
        }
      } else {
        astronautPos.y = Math.min(85, astronautPos.y + speed);
        moved = true;
      }
    }
    
    if (keys['ArrowLeft'] || keys['KeyA']) {
      if (astronautPos.x <= 2) {
        // At left boundary - scroll left if possible
        if (scrollLeft > 0) {
          window.scrollBy(-scrollSpeed, 0);
          moved = true;
        }
      } else {
        astronautPos.x = Math.max(2, astronautPos.x - speed);
        moved = true;
      }
    }
    
    if (keys['ArrowRight'] || keys['KeyD']) {
      if (astronautPos.x >= 85) {
        // At right boundary - scroll right if possible
        if (scrollLeft < documentWidth - windowWidth) {
          window.scrollBy(scrollSpeed, 0);
          moved = true;
        }
      } else {
        astronautPos.x = Math.min(85, astronautPos.x + speed);
        moved = true;
      }
    }
    
    if (moved) {
      positionAstronaut();
      isMoving = true;
      
      // Create particle trail occasionally
      if (Math.random() < 0.1) {
        createParticle();
      }
      
      // Hide hint when first moved
      if (hint && !hint.style.display) {
        hint.style.animation = 'hintFadeOut 0.5s ease-out forwards';
        setTimeout(() => {
          hint.style.display = 'none';
        }, 500);
      }
    } else {
      isMoving = false;
    }
  }
  
  // Enhance background star interaction
  function updateBackgroundStars() {
    if (!isMoving) return;
    
    const astronautRect = astronaut.getBoundingClientRect();
    const astronautCenterX = astronautRect.left + astronautRect.width / 2;
    const astronautCenterY = astronautRect.top + astronautRect.height / 2;
    
    bgStars.forEach(star => {
      const starRect = star.element.getBoundingClientRect();
      const starCenterX = starRect.left + starRect.width / 2;
      const starCenterY = starRect.top + starRect.height / 2;
      
      const distance = Math.sqrt(
        Math.pow(astronautCenterX - starCenterX, 2) + 
        Math.pow(astronautCenterY - starCenterY, 2)
      );
      
      if (distance < 50) {
        // Enhanced star proximity effect
        star.element.style.opacity = '0.6';
        star.element.style.transform = 'scale(1.5)';
        
        setTimeout(() => {
          star.element.style.opacity = '';
          star.element.style.transform = '';
        }, 200);
      }
    });
  }
  
  // Game loop
  function gameLoop() {
    updateAstronautPosition();
    updateBackgroundStars();
    requestAnimationFrame(gameLoop);
  }
  
  // Keyboard controls - immediate response
  document.addEventListener('keydown', (e) => {
    // Only respond to arrow keys and WASD
    if (['ArrowUp', 'ArrowDown', 'ArrowLeft', 'ArrowRight', 'KeyW', 'KeyA', 'KeyS', 'KeyD'].includes(e.code)) {
      keys[e.code] = true;
      e.preventDefault(); // Prevent scrolling
    }
  });
  
  document.addEventListener('keyup', (e) => {
    if (['ArrowUp', 'ArrowDown', 'ArrowLeft', 'ArrowRight', 'KeyW', 'KeyA', 'KeyS', 'KeyD'].includes(e.code)) {
      keys[e.code] = false;
    }
  });
  
  // Handle window blur/focus
  window.addEventListener('blur', () => {
    keys = {}; // Clear all keys when window loses focus
  });
  
  // Initialize when DOM is ready
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', init);
  } else {
    init();
  }
})();
    </script>
  </body>
</html>