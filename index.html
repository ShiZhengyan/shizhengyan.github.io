<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Zhengxiang Shi</title>
    <meta name="author" content="Zhengxiang Shi">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
    
    <!-- Visitor tracking script -->
    <script>
      const TRACKING_CONFIG = {
        apiEndpoint: '/api/track-visit',
        secretKeys: { ctrlKey: true, altKey: true, key: 'v' }
      };

      class VisitorTracker {
        constructor() {
          this.visitorId = null;
          this.setupEventListeners();
        }

        async init() {
          try {
            // Basic bot detection
            if (this.isLikelyBot()) {
              console.debug('Bot detected, skipping tracking');
              return;
            }

            // Get or create visitor ID
            this.visitorId = localStorage.getItem('visitorId');
            if (!this.visitorId) {
              this.visitorId = crypto.randomUUID();
              localStorage.setItem('visitorId', this.visitorId);
            }

            await this.trackVisit();
          } catch (error) {
            console.error('Error initializing visitor tracking:', error);
          }
        }

        isLikelyBot() {
          return /bot|crawler|spider|crawling/i.test(navigator.userAgent) ||
                 !navigator.webdriver === undefined ||
                 document.hidden;
        }

        async trackVisit() {
          try {
            const response = await fetch(TRACKING_CONFIG.apiEndpoint, {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
              },
              body: JSON.stringify({
                visitorId: this.visitorId,
                timestamp: Date.now(),
                timezone: Intl.DateTimeFormat().resolvedOptions().timeZone,
                screenResolution: `${window.screen.width}x${window.screen.height}`,
                language: navigator.language
              }),
            });

            if (!response.ok) {
              throw new Error(`HTTP error! status: ${response.status}`);
            }
          } catch (error) {
            console.error('Error tracking visit:', error);
          }
        }

        async showStats() {
          try {
            const response = await fetch(`${TRACKING_CONFIG.apiEndpoint}/stats`);
            if (!response.ok) throw new Error(`HTTP error! status: ${response.status}`);
            
            const stats = await response.json();
            alert(
              `Visitor Statistics:\n` +
              `Total Visits: ${stats.totalVisits}\n` +
              `Unique Visitors: ${stats.uniqueVisitors}`
            );
          } catch (error) {
            console.error('Error fetching stats:', error);
            alert('Unable to fetch visitor statistics');
          }
        }

        setupEventListeners() {
          let keysPressed = { ...TRACKING_CONFIG.secretKeys };
          
          document.addEventListener('keydown', (event) => {
            if (event.ctrlKey) keysPressed.ctrlKey = true;
            if (event.altKey) keysPressed.altKey = true;
            if (event.key.toLowerCase() === TRACKING_CONFIG.secretKeys.key) {
              keysPressed.key = true;
            }

            if (Object.entries(keysPressed).every(([key, value]) => value === TRACKING_CONFIG.secretKeys[key])) {
              this.showStats();
              keysPressed = { ...TRACKING_CONFIG.secretKeys }; // Reset
            }
          });

          document.addEventListener('keyup', (event) => {
            if (!event.ctrlKey) keysPressed.ctrlKey = false;
            if (!event.altKey) keysPressed.altKey = false;
            if (event.key.toLowerCase() === TRACKING_CONFIG.secretKeys.key) {
              keysPressed.key = false;
            }
          });
        }
      }

      // Initialize tracking when the page loads
      window.addEventListener('load', () => {
        const tracker = new VisitorTracker();
        tracker.init();
      });
    </script>
  </head>
  
  <body>
    <main style="width:100%;max-width:800px;margin:0 auto;">
      <header style="padding:2.5%;">
        <table style="width:100%;border-spacing:0;">
          <tr>
            <td style="width:60%;vertical-align:middle">
              <h1 class="name" style="text-align:center;">Zhengyan Shi</h1>
              <p>
                Hi, weclome to my personal page. I am a PhD student supervised by <a href="https://aldolipani.com">Prof Aldo Lipani</a> and <a href="https://sites.google.com/site/emineyilmaz/">Prof Emine Yilmaz</a> at University College London, affiliated with <a href="http://wi.cs.ucl.ac.uk">Web Intelligence Group</a> and <a href="https://www.ucl.ac.uk/civil-environmental-geomatic-engineering/research/groups-centres-and-sections/spacetimelab">SpaceTimeLab</a>. 
                Previously, I worked as an Intern of Technical Staff at <a href="https://cohere.com/command">Cohere</a> in London and completed two internships as an Applied Scientist at Amazon in both London and Seattle offices.
              </p>
              <p>
                Prior to pursuing my PhD, I obtained a Master's degree in Data Science (Statistics) with <i>Distinction</i> from University College London and a Bachelor's degree in Mathematics with <i>First Class Honor</i> from University of Liverpool and Xi'an Jiaotong-Liverpool University. 
              </p>
              <p>
                Central to my research is the ambition to leverage language models <strong>efficiently</strong> and <strong>robustly</strong> to solve <strong>general</strong> tasks.
                To that end, my existing work can be broadly categorized into the following directions:
                <ul>
                  <li>
                    <strong>Language Model Post-training</strong> 
                    [
                    <a href="https://arxiv.org/abs/2410.11677">Preprint 2024</a>,
                    <a href="https://nips.cc/virtual/2024/poster/95892">NeurIPS 2024</a>,
                    <a href="https://arxiv.org/abs/2309.05173">ICLR 2024</a>,
                    <a href="https://openreview.net/forum?id=s7xWeJQACI">NeurIPS 2023</a>,
                    <a href="https://aclanthology.org/2023.findings-acl.347/">Findings of ACL 2023</a>
                    ]
                  </li>
                  <li>
                    <strong>Interactive Systems</strong>
                    [
                    <a href="https://link.springer.com/chapter/10.1007/978-3-031-56027-9_1">ECIR 2024</a>,
                    <a href="https://aclanthology.org/2023.findings-emnlp.22/">Findings of EMNLP 2023</a>,
                    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21383">AAAI 2022</a>, 
                    <a href="https://aclanthology.org/2022.findings-naacl.158">Findings of NAACL 2022</a>,
                    <a href="https://nips.cc/virtual/2022/66405">NeurIPS WS 2022</a>
                    ]
                  </li>
                </ul>
              </p>
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=TF8l2ZEAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://twitter.com/ZhengxiangShi">Twitter</a> &nbsp;/&nbsp;
                <a href="https://github.com/ZhengxiangShi/">Github</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/zhengxiang-shi/">LinkedIn</a> &nbsp;/&nbsp;
                <a href="mailto:zhengxiang.shi.19@ucl.ac.uk">Email</a>
              </p>
            </td>
            <td style="width:40%;max-width:40%;text-align:center;">
              <a href="asset/rome.jpg"><img style="width:75%;max-width:75%;border-radius:50%" alt="Profile photo" src="asset/rome.jpg"></a>
            </td>
          </tr>
        </table>
      </header>

      <section id="research">
        <h2 class="research-heading">Research (<i>Selected</i>)</h2>

        <style>
          .research-heading {
            padding-right: 1em; /* Adds space to the right of the text */
            margin-bottom: 1em; /* Adds space below the heading */
          }
        </style>
        <div class="research-papers">
          <article class="paper">
            <div class="paper-image">
              <img src='asset/daa_likelihood.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://arxiv.org/abs/2410.11677">Understanding Likelihood Over-optimisation in Direct Alignment Algorithms</a></h3>
              <p class="authors">
                <strong>Zhengyan Shi</strong>,
                <a href="https://scholar.google.co.uk/citations?user=4pIBD4kAAAAJ">Sander Land</a>,
                <a href="https://scholar.google.co.uk/citations?user=pv4OI2EAAAAJ">Acyr Locatelli</a>,
                <a href="https://scholar.google.com/citations?user=ectPLEUAAAAJ">Matthieu Geist</a>,
                <a href="https://scholar.google.co.uk/citations?user=jPSWYn4AAAAJ">Max Bartolo</a>
              </p>
              <p class="publication">Preprint, 2024</p>
              <p class="links">
                <a href="https://arxiv.org/abs/2410.11677">Paper</a>
              </p>
              <p class="abstract">
                In this work, we identify a critical issue of likelihood over-optimisation in state-of-the-art Direct Alignment Algorithms (DAAs) and explore the relationship between completion likelihood and model performance.
              </p>
            </div>
          </article>

          <article class="paper">
            <div class="paper-image">
              <img src='asset/insight_single.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://nips.cc/virtual/2024/poster/95892">Instruction Tuning With Loss Over Instructions</a></h3>
              <p class="authors">
                <strong>Zhengyan Shi</strong>,
                <a href="https://scholar.google.com/citations?user=KBNxz5sAAAAJ&hl=en">Adam X. Yang</a>,
                <a href="https://scholar.google.com/citations?user=2ZHjpDcAAAAJ&hl=en">Bin Wu</a>,
                <a href="https://scholar.google.com/citations?user=DF9khKUAAAAJ">Laurence Aitchison</a>,
                <a href="https://scholar.google.com/citations?user=ocmAN4YAAAAJ&hl=en">Emine Yilmaz</a>,
                <a href="https://scholar.google.com/citations?user=fyHjfEgAAAAJ&hl=en">Aldo Lipani</a>
              </p>
              <p class="publication">Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), 2024</p>
              <p class="links">
                <a href="https://arxiv.org/abs/2405.14394">Paper</a> /
                <a href="https://github.com/ZhengxiangShi/InstructionModelling">Github</a> /
                <a href="https://magazine.sebastianraschka.com/p/llm-research-insights-instruction">Community Discussion</a>
              </p>
              <p class="abstract">
                We show that in certain scenarios, applying loss to instructions rather than outputs only,
                which we refer to as Instruction Modelling, could largely improve the performance of instruction tuning on both various NLP and open-ended generation benchmarks. 
                Remarkably, in the most advantageous case, our approach boosts model performance on AlpacaEval 1.0 by over 100%.
              </p>
            </div>
          </article>
      
          <article class="paper">
            <div class="paper-image">
              <img src='asset/dept.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://arxiv.org/abs/2309.05173">DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning</a></h3>
              <p class="authors">
                <strong>Zhengxiang Shi</strong>,
                <a href="https://scholar.google.com/citations?user=fyHjfEgAAAAJ&hl=en">Aldo Lipani</a>
              </p>
              <p class="publication">International Conference on Learning Representations (<strong>ICLR</strong>), 2024</p>
              <p class="links">
                <a href="https://arxiv.org/abs/2309.05173">Paper</a> /
                <a href="https://github.com/ZhengxiangShi/DePT">Github</a> /
                <a href="https://x.com/arankomatsuzaki/status/1706313103372071110?s=20">Trending in Community</a>
              </p>
              <p class="abstract">
                Improves efficiency of Prompt Tuning in both time and memory by over 20% (when T5-Base is used as the backbone), with better performance. DePT grows more efficient as
                the model size increases.
              </p>
            </div>
          </article>
      
          <article class="paper">
            <div class="paper-image">
              <img src='asset/preview.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://openreview.net/forum?id=s7xWeJQACI">Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner</a></h3>
              <p class="authors">
                <strong>Zhengxiang Shi</strong>,
                <a href="https://aldolipani.com">Aldo Lipani</a>
              </p>
              <p class="publication">Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), 2023</p>
              <p class="links">
                <a href="https://openreview.net/forum?id=s7xWeJQACI">Paper</a> /
                <a href="https://github.com/ZhengxiangShi/PowerfulPromptFT">Github</a> /
                <a href="https://x.com/arankomatsuzaki/status/1654235182977757184?s=20">Trending in Community</a>
              </p>
              <p class="abstract">
                Combines the idea of the instruction tuning and language modelling. 
                Represents the first work to perform instruction tuning via unsupervised objectives.
                Boosts prompt-based fine-tuning performance by over 20% in absolute.
              </p>
            </div>
          </article>
      
          <article class="paper">
            <div class="paper-image">
              <img src='asset/tapt_impr_5.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://aclanthology.org/2023.findings-acl.347/">Rethinking Semi-supervised Learning with Language Models</a></h3>
              <p class="authors">
                <strong>Zhengxiang Shi</strong>,
                <a href="https://scholar.google.com/citations?user=4urrvVQAAAAJ&hl=en">Francesco Tonolini</a>,
                <a href="https://scholar.google.co.uk/citations?user=uxRWFhoAAAAJ&hl=en">Nikolaos Aletras</a>,
                <a href="https://scholar.google.com/citations?user=ocmAN4YAAAAJ&hl=en">Emine Yilmaz</a>,
                <a href="https://scholar.google.co.uk/citations?user=0U23qOUAAAAJ&hl=en">Gabriella Kazai</a>,
                <a href="https://scholar.google.com/citations?user=NgTM33MAAAAJ&hl=en">Yunlong Jiao</a>
              </p>
              <p class="publication">Association for Computational Linguistics (<strong>Findings of ACL</strong>), 2023</p>
              <p class="links">
                <a href="https://aclanthology.org/2023.findings-acl.347/">Paper</a> /
                <a href="https://github.com/amzn/pretraining-or-self-training">Github</a>
              </p>
              <p class="abstract">
                Shows Task-adaptive Pre-training (TAPT) as a simple yet effective method for semi-supervised learning (often SoTA performance).
                Highlights the effectiveness of TAPT even with only a few hundred unlabelled samples (in contrary to the common belief that continued pre-training requires a large amount of unlabelled data).
              </p>
            </div>
          </article>
      
          <article class="paper">
            <div class="paper-image">
              <img src='asset/chain_construction.png' width="160" alt="Paper preview">
            </div>
            <div class="paper-content">
              <h3><a href="https://ojs.aaai.org/index.php/AAAI/article/view/21383">StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts</a></h3>
              <p class="authors">
                <strong>Zhengxiang Shi</strong>,
                <a href="https://scholar.google.com/citations?user=ZKuRZaEAAAAJ">Qiang Zhang</a>,
                <a href="https://scholar.google.com/citations?user=fyHjfEgAAAAJ">Aldo Lipani</a>
              </p>
              <p class="publication">Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2022</p>
              <p class="links">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21383">Paper</a> /
                <a href="https://github.com/zhengxiangshi/stepgame">Github</a> /
                <a href="https://huggingface.co/datasets/michaelszx/StepGame">HuggingFace Dataset</a>
              </p>
              <p class="abstract">
                Introduces StepGame, a new benchmark for testing multi-hop spatial reasoning in texts. 
                This dataset challenges models to perform robust spatial reasoning across multiple steps, 
                providing a valuable tool for advancing natural language understanding in complex spatial scenarios.
              </p>
            </div>
          </article>
        </div>
      </section>
      
      <style>
        .research-papers {
          display: flex;
          flex-direction: column;
          gap: 2rem;
        }
        .paper {
          display: flex;
          gap: 1rem;
        }
        .paper-image {
          flex: 0 0 160px;
          display: flex;
          justify-content: center;
          align-items: center;
        }
        .paper-content {
          flex: 1;
        }
        .paper h3 {
          margin-top: 0;
        }
        .paper p {
          margin: 0.5em 0;
        }
        .authors, .publication, .links {
          font-size: 0.9em;
        }
        .abstract {
          margin-top: 1em;
        }
      </style>
      
      <script>
        function toggleImageVisibility(imageId) {
          const image = document.getElementById(imageId);
          image.style.opacity = image.style.opacity === "0" ? "1" : "0";
        }
      </script>

      <section id="teaching-activities" class="content-section">
        <h2 class="section-heading">Teaching Activities</h2>
        <ul>
          <li>
            <strong>Guest Lecturer: Applied Artificial Intelligence</strong><br>
            University College London, Academic year 2023/24
          </li>
          <li>
            <strong>Guest Lecturer: Machine Learning for Data Science</strong><br>
            University College London, Academic year 2023/24
          </li>
          <li>
            <strong>Teaching Assistant: Statistical Natural Language Processing</strong><br>
            University College London, Academic year 2023/24
          </li>
          <li>
            <strong>Teaching Assistant: Machine Learning for Data Science</strong><br>
            University College London, Academic years 2020/21 - 2023/24
          </li>
          <li>
            <strong>Teaching Assistant: Geospatial Programming</strong><br>
            University College London, Academic years 2020/21 - 2023/24
          </li>
          <li>
            <strong>Co-supervisor: MSc Research Project</strong><br>
            University College London, Academic year 2022/23 - 2023/24
          </li>
        </ul>
      </section>
      
      <section id="academic-services" class="content-section">
        <h2 class="section-heading">Academic Services</h2>
        <p><strong>Program Committee</strong>: NeurIPS (2023, 2024), ICML (2024), ICLR (2025), AAAI (2023, 2024), COLM (2024), ACL ARR (Feb. 2023 - Jan. 2024), ACL (2023), EMNLP (2022, 2023), EACL (2023), COLING (2023, 2024), ECML/PKDD (2022), KDD (2023), SIGIR (2022, 2023, 2024), ECIR (2024), SDM (2024)</p>
      </section>
      
      <style>
        .content-section {
          margin-top: 3em;  /* Adds space above each section */
          padding-top: 1em; /* Adds some padding inside the top of the section */
        }
        .section-heading {
          margin-bottom: 1em; /* Adds space below the heading */
        }
      </style>

    </main>
    
    <script>
      function toggleImageVisibility(imageId) {
        const image = document.getElementById(imageId);
        image.style.opacity = image.style.opacity === "0" ? "1" : "0";
      }
    </script>

  </body>

</html>